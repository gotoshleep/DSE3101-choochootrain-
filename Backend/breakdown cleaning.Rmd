---
title: "Vulnerability Cleaning"
author: "ashley"
date: "`r Sys.Date()`"
output: html_document
---



```{r}
library(tidyverse)
library(jsonlite)
library(rvest)
library(lubridate)
library(stringr)
library(ggplot2)
library(tidytext)
library(jsonlite)
library(chron)
library(stringdist) 
#rm(list = ls())
```

# SMRT Updates
```{r loading JSON files}
path <- "raw_data/2017-March2025 mrt updates.json"
tele_data <- fromJSON(path) 

# we are extracting data and message columns so that easier to work with
cleaned_tele_data <- tele_data["messages"] %>% 
  as_tibble() %>%
  select(messages) %>% 
  unnest(messages) %>%
  filter(type == "message") %>% # only include the messages
  select(date, text) %>%
  # Handle the text column which contains lists
  mutate(
    # Combine all text elements into a single string
    message_text = map_chr(text, ~ {
      # For each element in text, check if it's a character or a list
      if (is.list(.x)) {
        # If it's a list (like a link), extract the "text" element
        paste(map_chr(.x, ~ ifelse(is.list(.x), .x$text, .x)), collapse = " ")
      } else {
        # If it's just character vectors, collapse them
        paste(.x, collapse = " ")
      }
    }),
    # Clean up any extra whitespace
    message_text = trimws(gsub("\\s+", " ", message_text))
  ) %>%
  mutate(
    date_only = as.Date(date),  # Extracts date (YYYY-MM-DD)
    time_only = as.times(format(ymd_hms(date), "%H:%M:%S")), # Extracts time (HH:MM:SS) 
    message_text = message_text
  )%>% 
  select(-text, -date) %>%
  filter(message_text != "")  # filter out those empty message_text

```

```{r Bag of words}
library(wordcloud)
cleaned_tele_data <- cleaned_tele_data %>%
  mutate(message_id = row_number())


domain_words <- c('smrt','train', 'service','bus', 'free')
tidy_df <- cleaned_tele_data %>%
  unnest_tokens(word, message_text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!word %in% domain_words)

bow <- tidy_df %>%
  count(message_id, word) %>%
  pivot_wider(names_from = word, values_from = n, values_fill = 0)

tidy_df %>%
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, max.words = 100)) #we can already see some words that we can potentially used to filter for breakdown events
```
In order for us to identify breakdown messages, we will employ the following NLP techniques in the code block below: 1) bag of words 2) Sentiment analysis.
What we sought to do is to first tokenise the messages, which will be to separate the text messages into words, then we will will the 
get_sentiments("bing")function from the tidytext package in R, it allows us to identify words that have a negative sentiment (we normally attribute train breakdowns to being negative). We also added words which we have identified to be synonymous with train breakdowns that may not be negative: recover, resolve etc.
Unfortunately, some of the reasons for long delays may be affected by other factors, so we have to avoid them too (public holidays, nearby buildings catching on fire)
From here we are also able to get the mrt stations affected, since the message follows a pattern e.g: from angmokio to bishan

```{r getting trainbreakdown messages and defining a breakdown}


top_words <- cleaned_tele_data %>%
  unnest_tokens(word, message_text) %>%
  filter(!word %in% stop_words$word) %>% # remove common english words (known as stop_words: is, and what not)
  count(word, sort = TRUE) %>%
  filter(!word %in% domain_words)



top_2wordedphrases <- cleaned_tele_data%>%
  unnest_tokens(
    output = phrase,  # New column name for phrases
    input = message_text,     # Column containing text 
    token = "ngrams", # Extract phrases
    n = 2             # Number of words in phrase (2 = bigrams)
  )%>%
  count(phrase, sort = TRUE)


top_3wordedphrases <- cleaned_tele_data %>%
  unnest_tokens(
    output = phrase,  # New column name for phrases
    input = message_text,     # Column containing text 
    token = "ngrams", # Extract phrases
    n = 3             # Number of words in phrase (2 = bigrams)
  )%>%
  count(phrase, sort = TRUE)

top_words %>% head(n = 200)
top_2wordedphrases%>% head(n = 200)
top_3wordedphrases%>% head(n = 200)
```



```{r}
bing_sentiments <- get_sentiments("bing")

negative_breakdown_words <-top_words %>%
  inner_join(bing_sentiments, by = "word") %>%
  filter(sentiment == "negative") %>%
  bind_rows(
    tibble(
      word = c("travel time", "repair", "repairing", "bypass", "delay", "breakdown", "add", "recover", "resolved", "due", "moving", "running", "due", "pls", "alternative", "normal", "restored", "resumed"), #some non-negative words we identified associated with breakdowns
      n = 0,                  # Default frequency 
      sentiment = NA_character_  # Default sentiment 
    )
  )
head(negative_breakdown_words, n= 20)
```



```{r}

# filter breakdown messages
pattern <- paste(negative_breakdown_words$word, collapse = "|")
mrt_breakdown <- cleaned_tele_data %>% 
  filter(str_detect(tolower(message_text), pattern)) %>% # 654 messages filtered out
  filter(!str_detect(tolower(message_text), "lrt|end earlier|holiday|fire|skip the bus|Staying out late tonight|new station|early closure|longer intervals|maintenance works|rail expansion works|detonation|signalling system checks|maintenance work|track improvement work|F1 Singapore Grand Prix")) %>% # 307 messages filtered out get rid of LRT, other reasons why theres a delay
  mutate(
  line_code = str_extract(message_text, "\\b[A-Z]{2,3}\\b")) %>% #getting the line code, str_extract will get values of 2/3 character long line
  mutate(
  line_code = case_when(
      line_code %in% c("NSL", "EWL", "NEL", "CCL", "DTL", "CGL", "TEL") ~ line_code,
      TRUE ~ NA),
  message_text = str_replace_all(message_text, "(#\\w+)\\s*/\\s*(#\\w+)", "\\1/\\2"), # handling special case #DhobyGhaut / #MarinaBay squish into one for ease

  # temporary column to isolate "from" and "to" stations 
  travel = tolower(str_extract(message_text, "(?i)(from|between|btwn|betwn|btw)\\s+(\\S+(?:\\s+\\S+)?)\\s+(to|and|&|towards)\\s+(\\S+(?:\\s+\\S+)?)")), # what this does is that it will take the phrases with the keywords before and after e.g from bishan to angmokio

  # Extract "from" station (first station after "from", "between", "btwn", "btwn")
  from_station = str_extract(travel, "(?<=(from|between|btwn|betwn|btw)\\s)(#?[A-Za-z0-9]+(?:\\s+(?!(to|and|&|towards)\\b)#?[A-Za-z0-9]+){0,1})") %>%
    gsub("^#", "", .), # remove hashtags
  
  # Extract "to" station (first station after "to", "and", "&")
  to_station = travel%>%
    str_extract("(?<=(to|and|&|towards)\\s)(?!from\\b|between\\b|btwn\\b|betwn\\b|btw\\b)(#?[A-Za-z0-9]+(?:\\s+#?[A-Za-z0-9]+)?)") %>%
                        gsub("^#", "", .),

  # Extract "cause" station (station after "fault at")
  cause_station = tolower(str_extract(message_text, "fault at\\s+(#?[A-Za-z0-9]+)")) %>%
    str_extract("#?[A-Za-z0-9]+$") %>%
    gsub("^#", "", .),
  
  # Extract train directions (station after "towards" or "twds")
  towards_station = tolower(str_extract(message_text, "(towards|twds)\\s+(#?[A-Za-z0-9]+)")) %>%
    str_extract("[A-Za-z]+\\d*$") %>%
    gsub("^#", "", .)
  ) 

```

We formally define a breakdown if there is a delay of more than 30 minutes, based on a previous analysis that was done on MRT breakdowns in the following article https://www.straitstimes.com/singapore/transport/major-mrt-breakdowns-double-as-overall-rail-reliability-remains-high.


Since we do not have the actual delay time, we will assume that breakdown messages will be sent continuously as long as there is a breakdown. We will split breakdowns into 3 different categories:
1) events that take days to resolve 
2) multiple events that happen in a day 
3) event that take 1 day to resolve

For this project, we will find the stations that are will be affected and disregard direction, this is because we want to know which specific stations are vulnerable, so as to help LTA identify areas that require more allocation of resources e.g: manpower.

We also plot the graph for the number of breakdowns throughout the years can noticeably see a large decrease from the peak in 2017, however we can note an increase in events happening from 2018-2024, while the dip in 2025 is due to the time of collecting the data (April) but it can be noted that we are on the track to (estimated) 21 breakdowns this year which is an increase from the last year.
```{r find breakdown events}
# multiple breaks in one day

# we retain only the earliest messages as they give us the most info needed (to from, line code) and we assume that train accidents typically happen within the day for ease of matching incidents (there are some instances where there are no update messages)
stations <- read.csv("raw_data/stations.csv")

days_where_one_or_more_lines_break <- mrt_breakdown %>%
  filter((date_only < as.Date("2024-09-25") | date_only > as.Date("2024-09-30")))%>%
  group_by(date_only) %>%
  filter(n_distinct(line_code[!is.na(line_code)]) >= 2) %>%  # see if there are days where 2 or more incidents happen on the same day, we assume a line can only breakdown once
  ungroup() %>%
  mutate(
    line_code = case_when(
      is.na(line_code) & date_only == as.Date("2017-11-16") ~ "NEL", 
      is.na(line_code) & date_only == as.Date("2019-05-14") & !grepl("BEDOK", message_text) ~ "NSL",  # NSL if 'BEDOK' is not in the message
      is.na(line_code) & grepl("bedok", message_text) ~ "DTL",  # DTL if 'BEDOK' is found in the message_text
      is.na(line_code) & date_only == as.Date("2020-08-20") ~ "EWL",
      TRUE ~ line_code  # Retain existing line_code if no conditions are met
    )
  )%>%
  group_by(date_only, line_code) %>%
  arrange(date_only,line_code, time_only) %>%
  mutate(time_rank = rank(time_only)) %>%
  filter(time_rank == min(time_rank, na.rm = TRUE) | time_rank == max(time_rank, na.rm = TRUE)) %>% 
  mutate(
    largest_rank = lead(time_only, default = NA),  # Get the previous time rank (lag)
    delay_duration = largest_rank - time_only,
    delay_duration = hms(delay_duration),
    delay_duration_minutes = hour(delay_duration)*60+minute(delay_duration)
  ) %>% 
  filter(time_rank == 1) %>%
  select(-largest_rank, -time_rank, -delay_duration) %>%
  filter(delay_duration_minutes >= 30) %>%
  ungroup() 

## major ewl breakdown in september: https://www.straitstimes.com/opinion/analysing-train-breakdowns-line-by-line
took_multiple_days_to_resolve <- mrt_breakdown %>% 
  group_by(date_only) %>%
  filter(date_only == as.Date("2024-09-25")
         ,time_only == min(time_only)) %>%
  mutate(
    delay_duration_minutes = 8400 #as_datetime("2024-10-01T05:27:00") - as_datetime("2024-09-25T10:09:21") 6 days!
  ) %>% 
  ungroup() 

one_line_break <- mrt_breakdown %>%
  group_by(date_only)%>% # we group by these attributes so that we can sort it amongst these common columns
  filter(n_distinct(line_code[!is.na(line_code)]) <= 1) %>% # theres some instance where the line_code is not mentioned so we include 0 and 1 distinct line_code
  arrange(date_only, time_only) %>%  # First, order by date and time
  mutate(time_rank = rank(time_only))  %>%
  filter(date_only < as.Date("2024-09-25") | date_only > as.Date("2024-09-30")) %>% # filter the long breakdown
  filter(time_rank == min(time_rank, na.rm = TRUE) | time_rank == max(time_rank, na.rm = TRUE)) %>% 
  mutate(
    largest_rank = lead(time_only, default = NA),  # Get the previous time rank (lag)
    delay_duration = largest_rank - time_only,
    delay_duration = hms(delay_duration),
    delay_duration_minutes = hour(delay_duration)*60+minute(delay_duration),
     from_station = case_when(date_only == as.Date("2024-10-01") ~ 'woodlands north', TRUE ~ from_station),
    to_station = case_when(date_only == as.Date("2024-10-01") ~ 'lentor', TRUE ~ to_station),
    line_code = case_when(is.na(line_code) & date_only == as.Date("2024-10-01") ~ "TEL", TRUE ~ line_code),
    line_code = case_when(is.na(line_code) & date_only == as.Date("2020-08-20") ~ "EWL", TRUE ~ line_code)
  ) %>% 
  filter(time_rank == 1) %>%
  select(-largest_rank, -time_rank, -delay_duration) %>%
  filter(delay_duration_minutes >= 30) %>%
  ungroup() 


breakdown_events_df <- bind_rows(   
  days_where_one_or_more_lines_break,
  took_multiple_days_to_resolve,
  one_line_break
)

```

save df for 6 months 
```{r}
#save(breakdown_events_df, file = "breakdown_events_df.RData")

```


Visualisation
```{r}
# all 8 years
breakdown_plot <- breakdown_events_df %>%
  mutate(year = year(as.Date(date_only))) %>%
  group_by(year) %>%
  summarise(breakdown_count = n())

ggplot(breakdown_plot, aes(x = year, y = breakdown_count)) +
  geom_line(color = "dodgerblue", size = 1.2) +
  geom_point(color = "darkblue", size = 3) +
  geom_text(aes(label = breakdown_count), vjust = -0.5, color = "black") +  # Adding text labels
  labs(
    title = "MRT Breakdown Incidents Over the Years",
    x = "Year",
    y = "Number of Breakdowns"
  ) +
  scale_x_continuous(breaks = breakdown_plot$year) +
  theme_minimal()

line_colours <- c(
  CGL = "darkolivegreen2",
  CCL = "orange",
  DTL = "darkslateblue",
  EWL = "mediumseagreen",
  NEL = "darkmagenta",
  NSL = "red3",
  TEL = "saddlebrown"
)

line_labels <- c(
  CGL = "Changi Airport Branch Line",
  CCL = "Circle Line",
  DTL = "Downtown Line",
  EWL = "East West Line",
  NEL = "North East Line",
  NSL = "North South Line",
  TEL = "Thomson East Coast Line"
)


# Prepare the data: add year column and count breakdowns by year and line
breakdown_bar_data <- breakdown_events_df %>%
  mutate(year = year(as.Date(date_only))) %>%
  group_by(year, line_code) %>%
  summarise(breakdown_count = n(), .groups = "drop")

# Create the bar chart
ggplot(breakdown_bar_data, aes(x = factor(year), y = breakdown_count, fill = line_code)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = line_colours, labels = line_labels) +
  labs(
    title = "MRT Breakdown Incidents by Line and Year",
    x = "Year",
    y = "Number of Breakdowns",
    fill = "MRT Line"
  ) +
  theme_minimal()


```




Because of how we are going to count the mrt stations that are affected by the breakdown, we want to include the opening date of each station so as to avoid including them.
```{r}
# need to add opening time so as to ensure that we do not accidentally include them in the breakdown event if it opens earlier, this data is from wiki
NSL_mrt_reference_names<- stations %>% 
  select(station_code, stations, line, line_code, line_number, join_station) %>%
  filter(line_code == "NSL") %>%
  mutate(opening_date = case_when(line_number >= 1 & line_number <= 4 ~ "1990-03-10"
                      ,line_number == 5 ~ "1996-02-10"
                      ,line_number >= 7 & line_number <= 11 ~ "1996-02-10"
                      ,line_number == 12 ~ "2019-11-02"
                      ,line_number >= 13 & line_number <= 14 ~ "1998-12-20"
                      ,line_number >= 15 & line_number <= 19 ~ "1987-11-07"
                      ,line_number >= 20 & line_number <= 26 ~ "1987-12-12"
                      ,line_number == 27 ~ "1998-11-04"
                      ,line_number == 28 ~ "2014-11-23"))

EWL_mrt_reference_names<- stations %>% 
  select(station_code, stations, line, line_code, line_number, join_station) %>%
  filter(line_code == "EWL") %>%
  mutate(opening_date = case_when(line_number >= 1 & line_number <= 3 ~ "1989-12-16"
                      ,line_number >= 4 & line_number <= 12 ~ "1989-11-04"
                      ,line_number >= 13 & line_number <= 16 ~ "1987-12-12"
                      ,line_number >= 17 & line_number <= 21 ~ "1988-03-12"
                      ,line_number == 22 ~ "2001-10-18"
                      ,line_number == 23 ~ "1988-03-12"
                      ,line_number >= 24 & line_number <= 26 ~ "1988-11-05"
                      ,line_number == 27 ~ "1990-07-06"
                      ,line_number >= 28 & line_number <= 29 ~ "2009-02-28"
                      ,line_number >= 30 & line_number <= 33 ~ "2017-06-18"))

NEL_mrt_reference_names<- stations %>% 
  select(station_code, stations, line, line_code, line_number, join_station) %>%
  filter(line_code == "NEL") %>%
  mutate(opening_date = case_when(line_number >= 1 & line_number <= 10 ~ "2003-06-20"
                      ,line_number == 11 ~ "2011-06-20"
                      ,line_number >= 12 & line_number <= 14 ~ "2003-06-20"
                      ,line_number == 15 ~ "2006-01-15"
                      ,line_number >= 16 & line_number <= 17 ~ "2003-06-20"
                      ,line_number == 18 ~ "2024-12-10"))

CCL_mrt_reference_names<- stations %>% 
  select(station_code, stations, line, line_code, line_number, join_station) %>%
  filter(line_code == "CCL") %>%
  mutate(opening_date = case_when(line_number >= 1 & line_number <= 11 ~ "2010-04-17"
                      ,line_number >= 12 & line_number <= 16 ~ "2009-05-28"
                      ,line_number == 17 ~ "2011-10-08"
                      ,line_number >= 19 & line_number <= 29 ~ "2011-10-08"
                      ,station_code == 'CE1' | station_code == 'CE1' ~ "2012-01-14"))

DTL_mrt_reference_names<- stations %>% 
  select(station_code, stations, line, line_code, line_number, join_station) %>%
  filter(line_code == "DTL") %>%
  mutate(opening_date = case_when(line_number >= 1 & line_number <= 3 ~ "2015-12-27"
                      ,line_number == 4 ~ "2025-02-28"
                      ,line_number >= 5 & line_number <= 13 ~ "2015-12-27"
                      ,line_number >= 14 & line_number <= 19 ~ "2013-12-22"
                      ,line_number >= 20 & line_number <= 35 ~ "2017-10-21"))

TEL_mrt_reference_names<- stations %>% 
  select(station_code, stations, line, line_code, line_number, join_station) %>%
  filter(line_code == "TEL") %>%
  mutate(opening_date = case_when(line_number >= 1 & line_number <= 3 ~ "2020-01-31"
                      ,line_number >= 4 & line_number <= 9 ~ "2021-08-28"
                      ,line_number >= 11 & line_number <= 20 ~ "2022-11-13"
                      ,line_number == 22 ~ "2022-11-13"
                      ,line_number >= 23 & line_number <= 29 ~ "2024-06-23"))

CGL_mrt_reference_names<- stations %>% 
  select(station_code, stations, line, line_code, line_number, join_station) %>%
  filter(line_code == "CGL") %>%
  mutate(opening_date = case_when(
                      line_number == 1 ~ "2001-01-10"
                      ,line_number == 2 ~ "2002-02-08" ))
                      

mrt_reference_names <- rbind(
  NSL_mrt_reference_names,
  EWL_mrt_reference_names,
  NEL_mrt_reference_names,
  CCL_mrt_reference_names,
  DTL_mrt_reference_names,
  TEL_mrt_reference_names,
  CGL_mrt_reference_names
) %>%
  mutate(
    opening_date = as.Date(opening_date)
  )

```



Because of how the inconsistent way the stations are mentioned in the tweets, we want to use fuzzy matching to match the stations mention with our reference data through fuzzy matching. Example: bkt png matches bukit panjang
```{r stations affected by staion breakdown}


# instances whereby stations are not specified in the tweet, we will assume all stations on the line are affected
to_from_na_values<-breakdown_events_df %>% 
  filter(is.na(to_station) | is.na(from_station)) %>%
  mutate(
    from_station = case_when(
      !is.na(cause_station) & is.na(from_station) ~ cause_station,
      date_only == as.Date("2018-03-01") ~ 'bukitpanjang',
      TRUE ~ from_station
    ),
    to_station = case_when(
      !is.na(towards_station) & is.na(to_station) ~ towards_station,
      TRUE ~ to_station  
    ),
    from_station = case_when(
      is.na(from_station) & !is.na(to_station) ~ to_station,
      TRUE ~ from_station
    ),
    to_station = case_when(
      is.na(to_station) & !is.na(from_station) ~ from_station,
      TRUE ~ to_station
    )
  ) 

#filter where stations not specified we will assume that the breakdown

nonNA_breakdown_events <- breakdown_events_df %>% 
  filter(!is.na(to_station) | !is.na(from_station))

nrow(breakdown_events_df) == nrow(nonNA_breakdown_events) + nrow(to_from_na_values) # check if the rows are correct

cleaned_breakdown_events <- rbind(to_from_na_values, nonNA_breakdown_events) %>%
  select(-travel, -cause_station, -towards_station) %>% 
  mutate(is_peak = case_when(as.numeric(trunc(time_only * 24)) > 6 & as.numeric(trunc(time_only * 24))  < 9 ~ 1,
                             as.numeric(trunc(time_only * 24)) > 16 & as.numeric(trunc(time_only * 24))  < 20 ~ 1,
                             TRUE ~ 0),
         day_type = case_when(wday(date_only, week_start = 1) %in% 6:7 ~ "WEEKEND/HOLIDAY",
                              TRUE ~ "WEEKDAY"))

head(cleaned_breakdown_events)



# Function to match mismatched to and from stations
match_station_to_code <- function(station_name, breakdown_line_code, reference_df, max_dist = 2) {
  if (is.na(station_name) || is.na(breakdown_line_code)) return(NA_character_)

  station_name <- tolower(trimws(station_name))

  line_stations <- reference_df %>%
    filter(line_code == breakdown_line_code) %>%
    mutate(distance = stringdist::stringdist(tolower(stations), station_name, method = "jw")) %>% ##Jaro-Winkler distance, which is a fuzzy string matching (naming formating not consistent)
    arrange(distance)

  best_match <- line_stations %>%
    filter(distance <= max_dist) %>%
    slice(1)

  if (nrow(best_match) == 0) {
    return(NA_character_)
  } else {
    return(best_match$station_code)
  }
}
cleaned_breakdown_events_final <- cleaned_breakdown_events %>%
  mutate(
    from_code = mapply(match_station_to_code, from_station, line_code, MoreArgs = list(reference_df = mrt_reference_names)),
    to_code = mapply(match_station_to_code, to_station, line_code, MoreArgs = list(reference_df = mrt_reference_names))
  ) %>%
  select(date_only, line_code, from_station, to_station,is_peak, day_type, from_code, to_code) %>%
  mutate(
    from_code_alpha = str_extract(from_code, "^[A-Za-z]+"),
    from_code_num = as.integer(str_extract(from_code, "\\d+$")),
    to_code_alpha = str_extract(to_code, "^[A-Za-z]+"),
    to_code_num = as.integer(str_extract(to_code, "\\d+$"))
  ) 

station_breakdown <- data.frame(
  date = as.Date(character()),
  line_code = character(),
  station_code = character(),
  is_peak = numeric(),
  day_type = character(),
  station_name = character(),
  stringsAsFactors = FALSE
)

# Pre-allocate list to store rows
rows_list <- vector("list", length = sum(table(cleaned_breakdown_events_final$line_code)))

row_index <- 1

for (i in 1:nrow(cleaned_breakdown_events_final)) {
  line_code <- cleaned_breakdown_events_final$line_code[i]
  date <- cleaned_breakdown_events_final$date_only[i]
  is_peak <- cleaned_breakdown_events_final$is_peak[i]
  day_type <- cleaned_breakdown_events_final$day_type[i]
  to_code_num <- cleaned_breakdown_events_final$to_code_num[i]
  from_code_num <- cleaned_breakdown_events_final$from_code_num[i]
  
  mrtstations <- mrt_reference_names[mrt_reference_names$line_code == line_code, ]

    if (is.na(to_code_num) | is.na(from_code_num)) {
    # If either to_code_num or from_code_num is NA, include all stations
    mrtstations_filtered <- mrtstations
  } else {
    # Otherwise, filter stations within the range of from_code_num and to_code_num
    mrtstations_filtered <- mrtstations[mrtstations$line_number >= min(to_code_num, from_code_num) & 
                                         mrtstations$line_number <= max(to_code_num, from_code_num), ]
  }

  for (j in 1:nrow(mrtstations_filtered)) {
    opening_date <-mrtstations_filtered$opening_date[j]
    station_code <- mrtstations_filtered$station_code[j]
    station_name <- mrtstations_filtered$stations[j]
    rows_list[[row_index]] <- data.frame(
      opening_date = opening_date,
      date = date,
      line_code = line_code,
      station_code = station_code,
      is_peak = is_peak,
      day_type = day_type,
      station_name = station_name,
      stringsAsFactors = FALSE
    )
    
    row_index <- row_index + 1
  }
}

# Combine all rows into a single data frame
station_breakdown <- do.call(rbind, rows_list) %>%
  mutate(
    year = year(date),
    month = month(date),
    day = day(date)
  )

write.csv(station_breakdown, "breakdownevents.csv")
#%>% filter(is.na(from_code) | is.na(to_code))
```



Because our data set is highly unbalanced, we will generate synthetic data for the static modelling. 
For every MRT breakdown date, we will generate permutations of is_peak and day_type for each station, we will filter out stations that are not built before

```{r}
library(readxl)
library(tidyr)
library(lubridate)

breakdowns <- read.csv("raw_data/breakdownevents.csv") %>% select(-X, -line_code) %>% rename("stations" = "station_name") %>% 
  mutate(day_type = case_when(day_type == 'WEEKEND/HOLIDAY'~ 'WEEKENDS/HOLIDAY', TRUE ~ day_type))
ridership <- read.csv("raw_data/ridership_by_stations.csv") %>% select(-line_code) %>% rename("day_type" = "DAY_TYPE", "n_riders" = "AVG_RIDERS") %>% mutate(is_peak =as.numeric(is_peak))
stations <- read.csv("raw_data/stations.csv")  %>% select(-X, -line_number, -join_station, -line) 
load("raw_data/mrt_rainfalldf_daily.RData")
rainfalldf <- rainfalldf %>% select(-join_station, -closest_device)

v_df_breakdown <- breakdowns %>% 
  mutate(date = as.Date(date)
         ,opening_date = as.Date(opening_date) ) %>%
  left_join(stations, by=c("stations", "station_code")) %>%
  left_join(rainfalldf, by = c("stations", "date"))  



###############################################################
###############################################################
#####creating dummy data for all dates that had breakdown######
###############################################################
###############################################################

all_dates <- unique(v_df_breakdown$date)
print(all_dates)
v_df_all <- v_df_breakdown %>% mutate(breakdown = 1)

for (i in 1:length(all_dates)) {
  Breakdown_date = all_dates[i]
  temp <- stations %>% 
    uncount(2) %>%   # Duplicate each row
    mutate(is_peak=rep(0:1, length.out = n())) %>% # the rows are labelled 0,1,0,1...
    mutate(date = as.Date(Breakdown_date))%>% 
    mutate(
      day_type = case_when(
        wday(date, week_start = 1) %in% 1:5 ~ "WEEKDAY",
        wday(date, week_start = 1) %in% 6:7 ~ "WEEKENDS/HOLIDAY") 
    )%>% ##assign weekday or weekend based on date
    mutate(year = year(date),
           month = month(date),
           day=day(date)) %>%
    left_join(rainfalldf, by = c("stations", "date")) 
  
  v_df_all <- bind_rows(v_df_all, temp %>% mutate(breakdown = 0))
}


v_df_all <- v_df_all %>% group_by(date, stations, station_code, is_peak) %>%
  summarise(across(everything(), ~first(.))) %>%
  ungroup() %>%
  mutate(line_age = floor(as.numeric(date - as.Date(commencement)) / 365.25),
         `rain_fall(mm)` = replace_na(`rain_fall(mm)`, 0)
  ) 


v_df_all<- v_df_all%>%
  left_join(ridership, by = c("day_type", "is_peak", "station_code"))  %>%
  filter(line_age >= 0)%>%
  group_by(date) %>%
  mutate(`rain_fall(mm)` = ifelse(is.na(`rain_fall(mm)`), mean(`rain_fall(mm)`, na.rm = TRUE), `rain_fall(mm)`)) %>%
  ungroup() 

v_df_all2<-v_df_all%>%
  left_join(mrt_reference_names %>% select(station_code, opening_date), by = c("station_code"))  %>%
  select(-opening_date.x) %>%
  rename("opening_date" = "opening_date.y") %>%
  filter (opening_date < date)



#save the file so that we can use it
#write.csv(v_df_all2, "vul_data_static.csv", row.names = FALSE)

#view(v_df_all)
```








