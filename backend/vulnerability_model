---
title: "Vulnerability Model"
author: "Choo Choo Train"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(xgboost)
library(Matrix)
library(caret) # to split data into test and training via random sampling
library(randomForest)
library(broom) # used for tidy() function
library(e1071) # for naiveBayes()
```


# Feature Engineering

NOTE: add_time not included, since it is more of a result of the breakdown (might be better for connectivity metric)

Use total_count for linear regression and XGBoost. Use vul_score (binary) for logit and Naive Bayes
```{r}
vul_data <- read.csv("vul_data.csv")

model_data <- vul_data %>%
  mutate(
    vul_score = case_when(total_count > 0 ~ 1, TRUE ~ 0), # indicator variable: station was involved in a service disruption
    
    cost_per_station = cost / n_stations, # average cost per station
    
    # Split morning and evening peak timings
    is_morning_peak = ifelse(is_peak & hour %in% 7:9, 1, 0),
    is_evening_peak = ifelse(is_peak & hour %in% 17:19, 1,0),
    
    # Number of lines for interchange stations
    n_lines = ifelse(is_interchange, 2, 1),
    n_lines = ifelse(stations %in% c("Dhoby Ghaut", "Marina Bay", "Outram Park"), 3, n_lines))

```

Variables:
* "line_code" -- identifier 
* "line_number" -- hypothesis: stations located in the middle are more susceptible, BUT this might over_fit the model 
* "is_interchange" -- hypothesis: interchange stations are more affected (but also more connected) due to many lines. 
* "is_above_ground" -- hypothesis: above ground are more susceptible to ba weather conditions, and are older
* is_above_ground*average_monthly_rainfall.mm. -- interaction term, XGB will include if it believes it is necessary
* is_above_ground*line_age -- interaction term
* "operator" -- operator: SMRT or SBS, not significant
* "line_age" -- hypothesis: older stations = more maintenance issues = more breakdowns, BUT might be an issue of whole lines instead of individual stations
* "n_stations" -- hypothesis: longer lines = more complex signalling system = more breakdowns 
* "length" -- similar to n_stations hypothesis
* "cost_per_km" -- hypothesis: higher cost = better quality system
* "cost_per_station" -- same as cost_per_km, will use cost_per_station instead since it is more intuitive
* "riders_per_station" -- hypothesis: higher ridership volume = more stress on train system = more breakdowns
* "n_riders" -- same as riders_per_station 
* "incidents_per_10k" -- scaled, number of incidents per 10k people 
* "causes_per_10k" -- scaled, number of times the train fault occured at the station itself
* "is_morning_peak" -- compare if morning or evening is worst (more for prediction model)
* "is_evening_peak" -- ^^
* "n_lines" -- number of lines the station services (1 to 3) 
* "average_monthly_rainfall.mm." -- average monthly rainfall in mm, hypothesis: higher rainfall = more people taking public transport = more stress on system = more breakdowns

# Setup 
```{r}

# Ensure consistent row counts
complete_rows <- complete.cases(model_data[, c("line_code", "line_number", "is_interchange", "is_above_ground", 
                                               "operator", "line_age", "n_stations", "length", "cost_per_station",
                                               "n_riders", "is_morning_peak", "is_evening_peak", "n_lines", 
                                               "average_monthly_rainfall.mm.")])


set.seed(1000)

# Stratified sampling: 80% train, 20% test
test_df <- model_data[complete_rows,] %>% group_by(vul_score) %>% sample_frac(0.2)
train_df <- subset(model_data[complete_rows,], !(X %in% test_df$X))

variables <- ~ line_code + 
  is_interchange + 
  is_above_ground +
  is_above_ground*average_monthly_rainfall.mm. +   # tracks more exposed to bad weather conditions
  is_above_ground*line_age +                       # older stations (EWL, NSL) are the oldest
  line_age + 
  operator + 
  n_stations + 
  cost_per_station + 
  n_riders + 
  n_lines + 
  is_morning_peak + 
  is_evening_peak +  
  average_monthly_rainfall.mm. -1                  # -1 to drop intercept, avoids collinearity.

# model.matrix() automatically one-hot encodes factors for XGBoosting
# sparse.model.matrix() used for high-cardinality (many factors) NOTE: XGB better after using sparse
x <- sparse.model.matrix(variables, data = model_data[complete_rows,])

x_train <- sparse.model.matrix(variables, data = train_df)
y_train_cont <- train_df$total_count
y_train_bin <- train_df$vul_score

x_test <- sparse.model.matrix(variables, data = test_df)
y_test_cont <- test_df$total_count
y_test_bin <- test_df$vul_score
```

# Exploratory Data Analysis
## Checking for correlation between variables
Among the numeric variables, we want to check for correlation between ridership volume (`n_riders`) and the average rainfall (`average_monthly_rainfall.mm.`) so see if higher ridership volume occurs when there is bad weather conditions. Our pairwise correlation plot shows a slight pattern, though it is not significant. Verifying this, we find that our two variables have a correlation of 0.045, which is very weak positive correlation. We can conclude that there is no correlation between these two variables. 
```{r}
x_rider_rainfall <- model.matrix(~ n_riders + average_monthly_rainfall.mm., 
                                 data = model_data[complete_rows, ])[, -1]      # remove intercept
pairs(x_rider_rainfall)
cor(x_rider_rainfall, method = c("pearson", "kendall", "spearman"))
```

```{r}

```


Across all numeric variables (continuous and binary), there does not appear to be any strong correlation between any of the variables.
```{r}
x_numeric <- model.matrix(~ is_interchange + is_above_ground + n_stations + cost_per_station + line_age + 
                            n_riders + n_lines + is_morning_peak + is_evening_peak + average_monthly_rainfall.mm.,
                          data = model_data[complete_rows, ])[, -1]    # remove intercept
cor(x_numeric, method = c("pearson", "kendall", "spearman"))
pairs(x_numeric)
```

For categorical variables, we observe that only Downtown Line and North East Line are operated by SBS Transit. All other lines are operated by SMRT Trains. Operator should not be a significant variable. 
```{r}
table(model_data$line_code, model_data$operator)
```
However, the assumption of independence amongst categorical variables cannot be guaranteed. For instance, age is highly correlated to line_code due to the commencement date of these lines.

## Distribution
Data is highly skewed with a large number of 0s for `total_count`. Few service disruptions have been recorded due to limitations in data. From the frequency table, we see that `total_counts` of more than 0 (counts of 1, 2 or 3) only make up 0.112% of all observations. This skewed distribution is something to account for when considering an appropriate model for predicting the vulnerability scores for each MRT station.
```{r}
hist(model_data$total_count, main = "Histogram of Service Disruption Counts", xlab = "Count")
lines(density(model_data$total_count))
table(model_data$total_count)
```



## XGBoost

model.matrix automatically one-hot encodes XGBoost since there might not be an ordinal relationship between certain categorical variables.

```{r}
# run XGBoost on training data
xgb_model <- xgboost(data = x_train, label = y_train_cont, 
                    nrounds = 100, objective = "reg:squarederror",
                    verbose = 0)
xgb_pred <- predict(xgb_model, x_test)

# Scale 0-1 scores
xgb_scaled <- (xgb_pred - min(xgb_pred))/(min(xgb_pred) - max(xgb_pred))
max(xgb_pred) # 0.8914484
```

Precision and recall: maximise recall (accuracy, the number of true positives across all actual true cases)
```{r}
xgb_pred_df <- data.frame(xgb_pred)
y_pred <- cbind(xgb_pred_df, y_test)
threshold <- y_pred %>% filter(y_test > 0) %>% min()

# Precision and Recall Metric: maximise recall as far as possible to identify stress areas
# find the lowest prediction score that correctly identifies an actual delayed station
TP <- y_pred %>% filter(predictions >= threshold & y_test_cont > 0) %>% count() %>% pull()
FN <- y_pred %>% filter(predictions < threshold & y_test_cont > 0) %>% count() %>% pull()
FP <- y_pred %>% filter(predictions >= threshold & y_test_cont == 0) %>% count() %>% pull()
precision <- TP / (TP + FP)     # insufficient data, and our recall is maximised by setting threshold
recall <- TP / (TP + FN)
F1 <- 2*precision*recall / (precision + recall)

precision # 0.001395187
```

However, RMSE is not a good metric to see how well our model is doing as it is highly skewed.
Will naturally be very low because the data has very few incident cases. Many entries are 0.
This would skew the data and treat everything as no service disruption.

`line_number` included: RMSE = 0.04031099
`line_number` excluded: RMSE = 0.03897016
As predicted, `line-number` causes overfitting

adding `line_code` is better than without
```{r}
# Calculate RMSE 
rmse_xgb <- sqrt(mean((y_test_cont - xgb_pred)^2)) # 0.0455 (unscaled)
rmse_xgb <- sqrt(mean((y_test_cont - xgb_scaled)^2)) # 0.0833 (scaled)
sigma_y <- sd(y_test) # 0.02767

baseline_rmse <- sqrt(mean((y_test_cont - mean(y_train))^2))
baseline_rmse # 0.04226829
```

Selected features by the XGB model
```{r}
# Selected features
importance <- xgb.importance(model = xgb_model)
selected_features <- importance[Gain > 0.01, Feature]  # Threshold
selected_features
# plot of feature importance
xgb.plot.importance(importance_matrix = importance, top_n = 10)
```
Cross validation
```{r}
# Cross-validation
xgb_cv <- xgb.cv(data = x_train,label = y_train_cont,
                 nrounds = 100, nfold = 5, objective = "reg:squarederror",
                 verbose = 0)
# Best iteration
best_iter <- which.min(xgb_cv$evaluation_log$test_rmse_mean) # 12 iterations

xgb_cv$evaluation_log[best_iter, c("train_rmse_mean", "train_rmse_std", "test_rmse_mean", "test_rmse_std")]
# test_rmse_mean = 0.05050087
# test_rmse_std = 0.01140707

# Plot RMSE learning curve
ggplot(xgb_cv$evaluation_log, aes(x = iter)) +
  geom_line(aes(y = train_rmse_mean, color = "Train")) +
  geom_line(aes(y = test_rmse_mean, color = "Test")) +
  labs(title = "Learning Curve", x = "Iteration", y = "RMSE") +
  theme_minimal()
```

 
```{r}
# Residual plot
plot_data <- data.frame(Predicted = xgb_pred, Residual = (y_test_cont-xgb_pred))
ggplot(plot_data, aes(x = Predicted, y = Residual)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residual Plot")

# Q-Q plot, looks okay
qqnorm(y_test_cont-xgb_pred)
qqline(y_test_cont-xgb_pred)
```


## Random Forest
Does not work well as we have an insufficient amount of breakdown incidences (606/39504). Random forest was unable to properly separate all the data and predicted most stations will not breakdown. Issue of data imbalance.

Random forest uses dense matrix.
```{r}

x_train_dense <- as.data.frame(as.matrix(x_train))
x_test_dense <- as.data.frame(as.matrix(x_test))

rf_ctrl <- trainControl(method = "cv", 
                        number = 5,
                        classProbs = TRUE,
                        summaryFunction = twoClassSummary)

rf_model <- train(x = x_train_dense,
                 y = factor(y_train_bin, levels = c(0, 1), labels = c("Class0", "Class1")),
                 method = "rf",
                 metric = "ROC",
                 trControl = rf_ctrl,
                 tuneLength = 5)  # Tests 5 different mtry values

# Best model
print(rf_model$finalModel)

rf_model$finalModel$votes
# Predictions
rf_pred <- as.data.frame(predict(rf_model, newdata = x_test, "prob"))
rf_pred[rf_pred$class1 > 0] # there are no observations (station characteristics) that have vul_score > 0 (i.e. breakdown incidence)
```


## Linear Regression
```{r}
# Fit lm model
lm_model <- lm(y_train_cont ~ x_train - 1)  # -1 removes intercept (already in model.matrix)

# Get vul_scores predictions
lm_score <- predict(lm_model, newdata = as.data.frame(x_test))

# Scale vul_score predictions to 0-1 range
lm_scaled <- (lm_score - min(lm_score)) / (max(lm_score) - min(lm_score))
max(lm_score) # 0.01105369

```
```{r}
# Model Evaluation
summary(lm_model)  # View coefficients and statistics

# Calculate RMSE 
rmse_lm <- sqrt(mean((y_test_cont - lm_score)^2)) # 0.0423 (unscaled)
rmse_lm <- sqrt(mean((y_test_cont - lm_scaled)^2)) # 0.404 (scaled) -- scaled since the max score is very low due to data distribution

# Plots
plot(lm_model)
```

Selected features by the Linear Regression model
```{r}
# Feature importance
tidy(lm_model) %>%
  mutate(term = reorder(term, abs(estimate))) %>% 
  arrange(desc(abs(estimate))) %>% 
  head(10) %>%
  ggplot(aes(x = abs(estimate), y = term)) +
  geom_col() +
  labs(title = "Feature Importance")
```


## Logistic Model
Performs similar to linear regression model
```{r}
# Fit logistic model
logit_model <- glm(y_train_bin ~ x_train - 1, family = binomial())

# Get probabilities
prob <- predict(logit_model, newdata = as.data.frame(x_test), type = "response")

# Scale 0-1
prob_scaled <- (prob - min(prob)) / (max(prob) - min(prob))
max(prob) # 0.0457
```

```{r}
# Model Evaluation
summary(logit_model)  # View coefficients and statistics

# Calculate RMSE
rmse_logit <- sqrt(mean((y_test_bin - prob)^2)) # 0.0320 (unscaled)
rmse_logit <- sqrt(mean((y_test_bin - prob_scaled)^2)) # 0.0667 (scaled)

# Plots: looks very bad lmao
plot(logit_model)
```

Selected features by Logistic model
```{r}
# Feature importance
tidy(logit_model) %>%
  mutate(term = reorder(term, abs(estimate))) %>% 
  arrange(desc(abs(estimate))) %>% 
  head(10) %>%
  ggplot(aes(x = abs(estimate), y = term)) +
  geom_col() +
  labs(title = "Feature Importance")
```


## Naive Bayes
Has strong independence assumptions, which may not hold for MRT data (better options are XGBoost and Random Forest)
Uses categorical variables (need to convert all numeric variables to categorical, which is arbitrary)
Seems good on paper, but violates assumptions on correlation between variables.
No information rate is high.
```{r}
y_train_factor <- as.factor(y_train_bin)
y_test_factor <- as.factor(y_test_bin)

# Train Naive Bayes model
nb_model <- naiveBayes(x_train, y_train_factor)

# Make predictions
nb_pred <- predict(nb_model, x_test)

# Confusion matrix to observe performance (precision and recall)
confusionMatrix(nb_predictions, y_test_factor)
```


# Finalise scores
Because of the incomplete and unbalanced data, XGBoost is the best, violates the least assumptions in our data.
```{r}
# Store XGBoost score to main dataset
model_data$xgb_score[complete_rows] <- predict(xgb_model, x)

# Scale xgb_scores from 0 to 1
min_score = min(model_data$xgb_score[complete_rows])
max_score = max(model_data$xgb_score[complete_rows])
model_data$xgb_scaled = (model_data$xgb_score - min_score) / (max_score - min_score)

# Verify
summary(model_data$xgb_scaled)

# Method: Quantile-based bins (equal distribution of stations)
model_data$vul_bucket_quantile <- cut(model_data$xgb_scaled,
                                     breaks = quantile(model_data$xgb_scaled, 
                                                     probs = seq(0, 1, 0.2), 
                                                     na.rm = TRUE),
                                     labels = c("Very Low", "Low", "Medium", "High", "Very High"),
                                     include.lowest = TRUE)

# Check frequency distribution
table(model_data$vul_bucket_quantile, useNA = "always")

# Visualize REDO
ggplot(model_data, aes(x = xgb_scaled, fill = vul_bucket_quantile)) +
  geom_histogram(bins = 30) +
  labs(title = "Vulnerability Score Distribution",
       x = "Scaled Vulnerability Score (0-1)",
       y = "Number of Stations")


vul_scores <- model_data %>% 
  dplyr::select(station_code, stations, line_code, month, day_type, hour, is_peak, xgb_score, xgb_scaled, vul_bucket_quantile) %>%
  rename(vul_category = vul_bucket_quantile)

vul_scores_agg <- vul_scores %>%
  group_by(station_code, stations, line_code) %>%
  summarise(avg_score = mean(xgb_scaled, na.rm = TRUE), .groups = 'drop') %>%
  mutate(vul_category = cut(avg_score,
                            breaks = quantile(avg_score, probs = seq(0, 1, 0.2), na.rm = TRUE),
                            labels = c("Very Low", "Low", "Medium", "High", "Very High"),
                            include.lowest = TRUE))


vul_scores_agg_time <- vul_scores %>%
  group_by(station_code, stations, line_code, day_type, is_peak) %>%
  summarise(avg_score = mean(xgb_scaled, na.rm = TRUE), .groups = 'drop') %>%
  mutate(vul_category = cut(avg_score,
                            breaks = quantile(avg_score, probs = seq(0, 1, 0.2), na.rm = TRUE),
                            labels = c("Very Low", "Low", "Medium", "High", "Very High"),
                            include.lowest = TRUE))

#write.csv(vul_scores, file = "vul_scores_full.csv")
#write.csv(vul_scores_agg, file = "vul_scores_agg.csv")
#write.csv(vul_scores_agg_time, file = "vul_scores_time.csv")
#write.csv(model_data, file = "vul_model_data.csv")
```

### Check for missing stations
```{r}
check <- vul_scores_agg %>% right_join(stations)
missing_scores <- check %>% filter(is.na(vul_category)) # 33
missing_scores

length(unique(stations$stations)) # 143
length(unique(vul_scores_agg$stations)) # 137

```




