library(tidyverse)
library(xgboost)
library(Matrix)
library(caret) # to split data into test and training via random sampling
library(randomForest)
library(broom) # used for tidy() function
library(e1071) # for naiveBayes()

# Feature engineering
vul_data <- read.csv("vul_data.csv")

model_data <- vul_data %>%
  mutate(
    vul_score = case_when(total_count > 0 ~ 1, TRUE ~ 0), # indicator variable: station was involved in a service disruption
    
    cost_per_station = cost / n_stations, # average cost per station
    
    # Split morning and evening peak timings
    is_morning_peak = ifelse(is_peak & hour %in% 7:9, 1, 0),
    is_evening_peak = ifelse(is_peak & hour %in% 17:19, 1,0),
    
    # Number of lines for interchange stations
    n_lines = ifelse(is_interchange, 2, 1),
    n_lines = ifelse(stations %in% c("Dhoby Ghaut", "Marina Bay", "Outram Park"), 3, n_lines))

# Filter for complete rows of relevant variables
complete_rows <- complete.cases(model_data[, c("line_code", "line_number", "is_interchange", "is_above_ground", 
                                               "operator", "line_age", "n_stations", "length", "cost_per_station",
                                               "n_riders", "is_morning_peak", "is_evening_peak", "n_lines", 
                                               "average_monthly_rainfall.mm.")])

set.seed(1000)

# Stratified sampling: 80% train, 20% test
test_df <- model_data[complete_rows,] %>% group_by(vul_score) %>% sample_frac(0.2)
train_df <- subset(model_data[complete_rows,], !(X %in% test_df$X))

variables <- ~ line_code + 
  is_interchange + 
  is_above_ground +
  is_above_ground*average_monthly_rainfall.mm. +   # tracks more exposed to bad weather conditions
  is_above_ground*line_age +                       # older stations (EWL, NSL) are the oldest
  line_age + 
  operator + 
  n_stations + 
  cost_per_station + 
  n_riders + 
  n_lines + 
  is_morning_peak + 
  is_evening_peak +  
  average_monthly_rainfall.mm. -1                  # -1 to drop intercept, avoids collinearity.

# sparse.model.matrix() used for high-cardinality (many factors)
# Automatically one-hot encodes factors (avoids ordinal relationships)
x <- sparse.model.matrix(variables, data = model_data[complete_rows,])

x_train <- sparse.model.matrix(variables, data = train_df)
y_train_cont <- train_df$total_count
y_train_bin <- train_df$vul_score

x_test <- sparse.model.matrix(variables, data = test_df)
y_test_cont <- test_df$total_count
y_test_bin <- test_df$vul_score

## Exploratory Data Analysis 
# Check for correlation between ridership volume and rainfall patterns
x_rider_rainfall <- model.matrix(~ n_riders + average_monthly_rainfall.mm., 
                                 data = model_data[complete_rows, ])[, -1]      # remove intercept
pairs(x_rider_rainfall)
cor(x_rider_rainfall, method = c("pearson", "kendall", "spearman"))

# Check for correlation between numeric variables
x_numeric <- model.matrix(~ is_interchange + is_above_ground + n_stations + cost_per_station + line_age + 
                            n_riders + n_lines + is_morning_peak + is_evening_peak + average_monthly_rainfall.mm.,
                          data = model_data[complete_rows, ])[, -1]    # remove intercept
cor(x_numeric, method = c("pearson", "kendall", "spearman"))
pairs(x_numeric)

# Check for patterns in MRT lines and operators
table(model_data$line_code, model_data$operator)

# Distribution of service disruption incidences
hist(model_data$total_count, main = "Histogram of Service Disruption Counts", xlab = "Count")
lines(density(model_data$total_count))
table(model_data$total_count)



## XGBoost
# Fit XGBoost model
xgb_model <- xgboost(data = x_train, label = y_train_cont, 
                     nrounds = 100, objective = "reg:squarederror",
                     verbose = 0)
# Make scoring predictions
xgb_pred <- predict(xgb_model, x_test)

# Scale predictions to 0-1 range
xgb_scaled <- (xgb_pred - min(xgb_pred))/(min(xgb_pred) - max(xgb_pred))

xgb_pred_df <- data.frame(xgb_pred)
y_pred <- cbind(xgb_pred_df, y_test)
threshold <- y_pred %>% filter(y_test > 0) %>% min()

# Precision and Recall Metric: maximise recall as far as possible to identify stress areas
TP <- y_pred %>% filter(predictions >= threshold & y_test_cont > 0) %>% count() %>% pull()
FN <- y_pred %>% filter(predictions < threshold & y_test_cont > 0) %>% count() %>% pull()
FP <- y_pred %>% filter(predictions >= threshold & y_test_cont == 0) %>% count() %>% pull()
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
F1 <- 2*precision*recall / (precision + recall) # F-test statistic

# Calculate RMSE 
rmse_xgb <- sqrt(mean((y_test_cont - xgb_pred)^2))
rmse_xgb <- sqrt(mean((y_test_cont - xgb_scaled)^2))
# baseline RMSE using mean model for comparison
baseline_rmse <- sqrt(mean((y_test_cont - mean(y_train))^2))
baseline_rmse

# Selected features
importance <- xgb.importance(model = xgb_model)
selected_features <- importance[Gain > 0.01, Feature]  # set threshold for important features

# Plot feature importance of XGBoost model
xgb.plot.importance(importance_matrix = importance, top_n = 10)

# Cross-validation
xgb_cv <- xgb.cv(data = x_train,label = y_train_cont,
                 nrounds = 100, nfold = 5, objective = "reg:squarederror",
                 verbose = 0)

# Best iteration
best_iter <- which.min(xgb_cv$evaluation_log$test_rmse_mean)
xgb_cv$evaluation_log[best_iter, c("train_rmse_mean", "train_rmse_std", "test_rmse_mean", "test_rmse_std")]

# Plot RMSE learning curve
ggplot(xgb_cv$evaluation_log, aes(x = iter)) +
  geom_line(aes(y = train_rmse_mean, color = "Train")) +
  geom_line(aes(y = test_rmse_mean, color = "Test")) +
  labs(title = "Learning Curve", x = "Iteration", y = "RMSE") +
  theme_minimal()

# Residual plot
plot_data <- data.frame(Predicted = xgb_pred, Residual = (y_test_cont-xgb_pred))
ggplot(plot_data, aes(x = Predicted, y = Residual)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residual Plot")

# Q-Q plot
qqnorm(y_test_cont-xgb_pred)
qqline(y_test_cont-xgb_pred)



## Random Forest
x_train_dense <- as.data.frame(as.matrix(x_train)) # Random Forest uses dense matrices
x_test_dense <- as.data.frame(as.matrix(x_test))

rf_ctrl <- trainControl(method = "cv", 
                        number = 5,
                        classProbs = TRUE,
                        summaryFunction = twoClassSummary)

rf_model <- train(x = x_train_dense,
                  y = factor(y_train_bin, levels = c(0, 1), labels = c("Class0", "Class1")),
                  method = "rf",
                  metric = "ROC",
                  trControl = rf_ctrl,
                  tuneLength = 5)  # granularity

# Best model by Random Forest
rf_model$finalModel
rf_model$finalModel$votes

# Make scoring predictions
rf_pred <- as.data.frame(predict(rf_model, newdata = x_test, "prob"))
rf_pred[rf_pred$class1 > 0]



## Linear Regression Model
# Fit lm model
lm_model <- lm(y_train_cont ~ x_train - 1)  # -1 removes intercept (already in model.matrix)

# Make scoring predictions
lm_score <- predict(lm_model, newdata = as.data.frame(x_test))

# Scale predictions to 0-1 range
lm_scaled <- (lm_score - min(lm_score)) / (max(lm_score) - min(lm_score))

# Model evaluation: coefficients and statistics
summary(lm_model)

# Calculate RMSE 
rmse_lm <- sqrt(mean((y_test_cont - lm_score)^2))
rmse_lm <- sqrt(mean((y_test_cont - lm_scaled)^2))

plot(lm_model)

# Plot feature importance for linear regression model
tidy(lm_model) %>%
  mutate(term = reorder(term, abs(estimate))) %>% 
  arrange(desc(abs(estimate))) %>% 
  head(10) %>%
  ggplot(aes(x = abs(estimate), y = term)) +
  geom_col() +
  labs(title = "Feature Importance")



## Logistic Model
# Fit logistic model
logit_model <- glm(y_train_bin ~ x_train - 1, family = binomial())

# Make scoring predictions
prob <- predict(logit_model, newdata = as.data.frame(x_test), type = "response")

# Scale predictions to 0-1 range
prob_scaled <- (prob - min(prob)) / (max(prob) - min(prob))

# Model evaluation: coefficients and statistics
summary(logit_model)

# Calculate RMSE
rmse_logit <- sqrt(mean((y_test_bin - prob)^2))
rmse_logit <- sqrt(mean((y_test_bin - prob_scaled)^2))

plot(logit_model)

# Plot feature importance of logit model
tidy(logit_model) %>%
  mutate(term = reorder(term, abs(estimate))) %>% 
  arrange(desc(abs(estimate))) %>% 
  head(10) %>%
  ggplot(aes(x = abs(estimate), y = term)) +
  geom_col() +
  labs(title = "Feature Importance")



## Naive Bayes model
y_train_factor <- as.factor(y_train_bin)
y_test_factor <- as.factor(y_test_bin)

# Fit Naive Bayes model
nb_model <- naiveBayes(x_train, y_train_factor)

# Make scoring predictions
nb_pred <- predict(nb_model, x_test)

# Confusion matrix to observe performance
confusionMatrix(nb_predictions, y_test_factor)



# Finalised model using XGBoost model
# Store XGBoost score in main dataset
model_data$xgb_score[complete_rows] <- predict(xgb_model, x)

# Scale xgb_scores from 0 to 1 for easier interpretation
min_score = min(model_data$xgb_score[complete_rows])
max_score = max(model_data$xgb_score[complete_rows])
model_data$xgb_scaled = (model_data$xgb_score - min_score) / (max_score - min_score)

# Verify that XGBoost scores have been scaled
summary(model_data$xgb_scaled)

# Quantile-based bins (equal distribution of stations based on relative XGBoost scores)
model_data$vul_bucket_quantile <- cut(model_data$xgb_scaled,
                                      breaks = quantile(model_data$xgb_scaled, 
                                                        probs = seq(0, 1, 0.2), 
                                                        na.rm = TRUE),
                                      labels = c("Very Low", "Low", "Medium", "High", "Very High"),
                                      include.lowest = TRUE)

# Check frequency distribution
table(model_data$vul_bucket_quantile, useNA = "always")

# Visualise distribution of scores
ggplot(model_data, aes(x = xgb_scaled, fill = vul_bucket_quantile)) +
  geom_histogram(bins = 30) +
  labs(title = "Vulnerability Score Distribution",
       x = "Scaled Vulnerability Score (0-1)",
       y = "Number of Stations")

vul_scores <- model_data %>% 
  dplyr::select(station_code, stations, line_code, month, day_type, hour, is_peak, xgb_score, xgb_scaled, vul_bucket_quantile) %>%
  rename(vul_category = vul_bucket_quantile)

# Average scores across each MRT Line
vul_scores_line <- vul_scores %>%
  group_by(line_code) %>%
  summarise(avg_score = mean(xgb_scaled, na.rm = TRUE), .groups = 'drop') %>%
  mutate(vul_category = cut(avg_score,
                            breaks = quantile(avg_score, probs = seq(0, 1, 0.2), na.rm = TRUE),
                            labels = c("Very Low", "Low", "Medium", "High", "Very High"),
                            include.lowest = TRUE))

# Average scores of each MRT station and line (interchange stations have different scores depending on their line)
vul_scores_agg <- vul_scores %>%
  group_by(station_code, stations, line_code) %>%
  summarise(avg_score = mean(xgb_scaled, na.rm = TRUE), .groups = 'drop') %>%
  mutate(avg_score = case_when( # Fix missing entries for DTL 
    stations %in% c("Bukit Panjang", "Hume") ~ vul_scores_line$avg_score[vul_scores_line$line_code == "DTL"],
    TRUE ~ avg_score)) %>%
  mutate(vul_category = cut(avg_score,
                            breaks = quantile(avg_score, probs = seq(0, 1, 0.2), na.rm = TRUE),
                            labels = c("Very Low", "Low", "Medium", "High", "Very High"),
                            include.lowest = TRUE))

# Average scores of each MRT station and line by day_type (Weekday or Weekend/Holiday) and is_peak (peak or off-peak timings)
vul_scores_agg_time <- vul_scores %>%
  group_by(station_code, stations, line_code, day_type, is_peak) %>%
  summarise(avg_score = mean(xgb_scaled, na.rm = TRUE), .groups = 'drop') %>%
  mutate(vul_category = cut(avg_score,
                            breaks = quantile(avg_score, probs = seq(0, 1, 0.2), na.rm = TRUE),
                            labels = c("Very Low", "Low", "Medium", "High", "Very High"),
                            include.lowest = TRUE))
